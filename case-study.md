# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: 
- общее время обработки файла из 500 строк снижается

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: *как вы построили feedback_loop*

## Вникаем в детали системы, чтобы найти главные точки роста
Первое наблюдение:
- файл сначала считывается в память, а потом одной большой строкой сплитится чтобы разделить на строки
- коллекции перебирается 7 раз одинаковым образом


Для того, чтобы найти "точки роста" для оптимизации я воспользовался rbsy, stackprof, ruby-prof, benchmark.
Объединил методы файла в класс Parser и сделал клона ParserOptimized который я буду оптимизировать и сравнивать через Benchmark.bmbm с оригинальным кодом.


Вот какие проблемы удалось найти и решить

### Ваша находка №1
- ruby-prof (graph-html) показал, что метод `collect_stats_from_users` вызывается 7 раз и занимает 37.63% времени. Половину от короторого (16.74%) занимает `Date#parse`. Начнём с него
- Заменю на `Date#strptime` и укажу формат даты для парсера. Так он работает быстрее.
- Среднее время обработки файла в 500 строк снизилось примерно на 20%.
- Отчёт профилировщика показал что на парсинг стало уходить значительно меньше времени, но теперь стало видно что `Array#all?` вызывается 500 раз занимает примерно 20% общего времени

### Ваша находка №2
- `Array#all?` занимает почти 20% общего времени и был вызван 500 раз. Похоже что самопальный `uniq` для `uniqueBrowsers` делает много лишнего.
- Заменил на uniq
- общее время выполнения уменьшилось примерно на 10%. uniq при этом занимает всего 0.37% общего времени.
- Теперь стало сильнее выпячивать `Array#all?` и `Array.select` в подсчёте кол-ва уник браузеров и стате по пользователям.

### Ваша находка №3
- `Array#select` занимает 36% общего времени и самый тяжелый кажется тот что в поиске сессий юзера, потому что для каждого юзера пробегается по всему набору сессий.
- Сделаю сначала группировку через group_by, а потом буду просто по хэшу обращаться
- Общее время уменьшилось примерно на 35%! Круто, но в целом ожидаемо.
- 

### Ваша находка №4
- `Array#map` занимает 36% общего времени. Оно и понятно много лишних пробегов.
- Уберу цепочки `map {}.map {}` там где возможно. Объединить все вызовы collect_stats_from_users в один (-7 итераци:))
- Прирост не значительный, примерно 5-10%.
- Кажется я упёрся в `Array#split` и `Date#strptime`.

### Ваша находка №5
- всё тот же ruby-prof пока справляется с задаче и говорит что `Array#split` занимает 18% времени
- split сложно исключить везде, но в ruby 2.6 появился `split(..) {}` а это значит, что можно с пользой занять эту итерацию и перенести в неё парсинг строк. Плюс убрать лишнюю разбивку в `parse_user` и `parse_session` 
- Прирост незначительный... захожу в тупик
- Кажется что отчёт не сильно поменялся. Откатывать не буду, потому что на бОльших объёмах это будут лишние итерации.

### Ваша находка №6
- теперь видно что `#collect_stats_from_users` в котором почти вся стата 
- split сложно исключить везде, но в ruby 2.6 появился `split(..) {}` а это значит, что можно с пользой занять эту итерацию и перенести в неё парсинг строк. Плюс убрать лишнюю разбивку в `parse_user` и `parse_session` 
- Прирост незначительный... захожу в тупик
- теперь видно что `#collect_stats_from_users` в котором почти вся стата 

### Ваша находка №7
- Решил попробовать CallStack. Удобнее. Показывает примерно около 5-10% съедает парсинг дат. Не самый большой процент, но на больших объёмах может значительно вырасти.
- Так как парсинг даты дорогой, а даты могу повторяться, решил просто кэшировать распарсеные даты. Также сразу привожу к iso8601. Это используется только в обном месте, поэтому кажется самое то.
- Прирост примерно 5%, а `strptime` стал занимать всего 3.7% вместо 9! 
- 14.82% (25.55%) String#split [1000 calls, 1000 total] Что с ним сделать пока не придумал.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*

